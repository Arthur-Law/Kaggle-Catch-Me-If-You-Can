{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras and Tensorflow to build a MLP  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and data processing\n",
    "\n",
    "First, we will load the data and process it into a form that can be fed into our MLP neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figre_formats='svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data sets\n",
    "train_df = pd.read_csv(r'C:\\Users\\Law Wen Yu\\.jupyter\\data\\cmiyc\\train_sessions.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\Law Wen Yu\\.jupyter\\data\\cmiyc\\test_sessions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21668</th>\n",
       "      <td>21669</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54842</th>\n",
       "      <td>54843</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77291</th>\n",
       "      <td>77292</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114020</th>\n",
       "      <td>114021</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146669</th>\n",
       "      <td>146670</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id  site1               time1  site2               time2  \\\n",
       "21668        21669     56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57   \n",
       "54842        54843     56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   \n",
       "77291        77292    946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14   \n",
       "114020      114021    945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17   \n",
       "146669      146670    947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20   \n",
       "\n",
       "        site3               time3  site4               time4  site5   ...    \\\n",
       "21668     NaN                 NaT    NaN                 NaT    NaN   ...     \n",
       "54842    56.0 2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   ...     \n",
       "77291   951.0 2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   ...     \n",
       "114020  949.0 2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   ...     \n",
       "146669  948.0 2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   ...     \n",
       "\n",
       "                     time6  site7               time7  site8  \\\n",
       "21668                  NaT    NaN                 NaT    NaN   \n",
       "54842                  NaT    NaN                 NaT    NaN   \n",
       "77291  2013-01-12 08:50:16  948.0 2013-01-12 08:50:16  784.0   \n",
       "114020 2013-01-12 08:50:18  947.0 2013-01-12 08:50:19  945.0   \n",
       "146669 2013-01-12 08:50:21  946.0 2013-01-12 08:50:21  951.0   \n",
       "\n",
       "                     time8  site9               time9  site10  \\\n",
       "21668                  NaT    NaN                 NaT     NaN   \n",
       "54842                  NaT    NaN                 NaT     NaN   \n",
       "77291  2013-01-12 08:50:16  949.0 2013-01-12 08:50:17   946.0   \n",
       "114020 2013-01-12 08:50:19  946.0 2013-01-12 08:50:19   946.0   \n",
       "146669 2013-01-12 08:50:22  946.0 2013-01-12 08:50:22   947.0   \n",
       "\n",
       "                    time10  target  \n",
       "21668                  NaT       0  \n",
       "54842                  NaT       0  \n",
       "77291  2013-01-12 08:50:17       0  \n",
       "114020 2013-01-12 08:50:20       0  \n",
       "146669 2013-01-12 08:50:22       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df.sort_values(by='time1', inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websites total: 48371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35429</th>\n",
       "      <td>damkool.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>www.compteur-visite.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18488</th>\n",
       "      <td>lieuxsacres.canalblog.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24547</th>\n",
       "      <td>stockage.univ-brest.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41836</th>\n",
       "      <td>intranet.crfclermont.fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            site\n",
       "35429                damkool.com\n",
       "7849     www.compteur-visite.com\n",
       "18488  lieuxsacres.canalblog.com\n",
       "24547     stockage.univ-brest.fr\n",
       "41836    intranet.crfclermont.fr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change site1, ... , site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "\n",
    "# For the empty sites replace NaN with 0 and change type to int\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int')\n",
    "\n",
    "# Load website dictionary\n",
    "with open(r'C:\\Users\\Law Wen Yu\\.jupyter\\data\\cmiyc\\site_dic.pkl', 'rb') as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "    \n",
    "# Create dataframe for the dictionary\n",
    "site_dict = pd.DataFrame(list(site_dict.keys()),\n",
    "                        index=list(site_dict.values()), columns=['site'])\n",
    "print(u'Websites total:', site_dict.shape[0])\n",
    "site_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target variable \n",
    "y_train = train_df['target']\n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# Index to split the training and test set\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21668</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54842</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77291</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>784</td>\n",
       "      <td>949</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114020</th>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>949</td>\n",
       "      <td>948</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146669</th>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>948</td>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>952</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        site1  site2  site3  site4  site5  site6  site7  site8  site9  site10\n",
       "21668      56     55      0      0      0      0      0      0      0       0\n",
       "54842      56     55     56     55      0      0      0      0      0       0\n",
       "77291     946    946    951    946    946    945    948    784    949     946\n",
       "114020    945    948    949    948    945    946    947    945    946     946\n",
       "146669    947    950    948    947    950    952    946    951    946     947"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new df with only the visited sites\n",
    "full_sites = full_df[sites]\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_flatten = full_sites.values.flatten()\n",
    "full_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n",
    "                               sites_flatten,\n",
    "                               range(0, sites_flatten.shape[0] + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                            target='target', index_label='session_id'):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                               index = np.arange(1,\n",
    "                                                predicted_labels.shape[0] + 1),\n",
    "                               columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Next, let's create some relevant and interesting features. We will skip doing EDA since we have already done it in our previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>minutes</th>\n",
       "      <th>start_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>start_week</th>\n",
       "      <th>start_day</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>dow</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>work_hours</th>\n",
       "      <th>morning</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21668</th>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>201301</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>201302</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54842</th>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>29.77</td>\n",
       "      <td>201301</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>201302</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77291</th>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0.07</td>\n",
       "      <td>201301</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>201302</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114020</th>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0.05</td>\n",
       "      <td>201301</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>201302</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146669</th>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>201301</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>201302</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       min                 max  minutes  start_month  year  \\\n",
       "21668  2013-01-12 08:05:57 2013-01-12 08:05:57     0.00       201301  2013   \n",
       "54842  2013-01-12 08:37:23 2013-01-12 09:07:09    29.77       201301  2013   \n",
       "77291  2013-01-12 08:50:13 2013-01-12 08:50:17     0.07       201301  2013   \n",
       "114020 2013-01-12 08:50:17 2013-01-12 08:50:20     0.05       201301  2013   \n",
       "146669 2013-01-12 08:50:20 2013-01-12 08:50:22     0.03       201301  2013   \n",
       "\n",
       "        month  start_week  start_day  start_hour  dow  is_weekend  work_hours  \\\n",
       "21668       1      201302         12           8    5           1           0   \n",
       "54842       1      201302         12           8    5           1           0   \n",
       "77291       1      201302         12           8    5           1           0   \n",
       "114020      1      201302         12           8    5           1           0   \n",
       "146669      1      201302         12           8    5           1           0   \n",
       "\n",
       "        morning  day  evening  night  target  \n",
       "21668         1    0        0      0       0  \n",
       "54842         1    0        0      0       0  \n",
       "77291         1    0        0      0       0  \n",
       "114020        1    0        0      0       0  \n",
       "146669        1    0        0      0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df = pd.DataFrame(index=full_df.index)\n",
    "\n",
    "time_df['min'] = full_df[times].min(axis=1)\n",
    "time_df['max'] = full_df[times].max(axis=1)\n",
    "time_df['minutes'] = round((time_df['max'] - time_df['min']).astype('timedelta64[s]')/60,2)\n",
    "\n",
    "time_df['start_month'] = time_df['min'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "time_df['year'] = time_df['min'].apply(lambda ts: ts.year)\n",
    "time_df['month'] = time_df['min'].apply(lambda ts: ts.month)\n",
    "\n",
    "time_df['start_week'] = time_df['min'].apply(lambda ts: 100 * ts.year + ts.week)\n",
    "time_df['start_day'] = time_df['min'].apply(lambda ts: ts.timetuple().tm_yday)\n",
    "time_df['start_hour'] = time_df['min'].apply(lambda ts: ts.hour)\n",
    "\n",
    "time_df['dow'] = time_df['min'].apply(lambda ts: ts.date().weekday())\n",
    "time_df['is_weekend'] = time_df['min'].apply(lambda ts: 1 if ts.date().weekday() in (5,6) else 0)\n",
    "time_df['work_hours'] = time_df['min'].apply(lambda ts: 1 if (ts.date().weekday() in (0,1,2,3))\n",
    "                                            & ((ts.hour>=8)&(ts.hour<=17)&(ts.hour!=12)) else 0)\n",
    "\n",
    "hour = time_df['min'].apply(lambda ts: ts.hour)\n",
    "time_df['morning'] = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "time_df['day'] = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "time_df['evening'] = ((hour >=19) & (hour <= 23)).astype('int')\n",
    "time_df['night'] = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "\n",
    "time_df['target'] = y_train\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregrate based on the length of the session\n",
    "short = (time_df['minutes'] <= 1).astype('int')\n",
    "long = (time_df['minutes'] > 1).astype('int')\n",
    "\n",
    "# Segregrate based on day of week\n",
    "mon = (time_df['dow']==0).astype('int')\n",
    "tue = (time_df['dow']==1).astype('int')\n",
    "wed = (time_df['dow']==2).astype('int')\n",
    "thurs = (time_df['dow']==3).astype('int')\n",
    "fri = (time_df['dow']==4).astype('int')\n",
    "sat = (time_df['dow']==5).astype('int')\n",
    "sun = (time_df['dow']==6).astype('int')\n",
    "\n",
    "# Segregrate based on month\n",
    "jan = (time_df['month']==1).astype('int')\n",
    "feb = (time_df['month']==2).astype('int')\n",
    "mar = (time_df['month']==3).astype('int')\n",
    "apr = (time_df['month']==4).astype('int')\n",
    "may = (time_df['month']==5).astype('int')\n",
    "june = (time_df['month']==6).astype('int')\n",
    "july = (time_df['month']==7).astype('int')\n",
    "aug = (time_df['month']==8).astype('int')\n",
    "sep = (time_df['month']==9).astype('int')\n",
    "oct = (time_df['month']==10).astype('int')\n",
    "nov = (time_df['month']==11).astype('int')\n",
    "dec = (time_df['month']==12).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with 8 features\n",
    "\n",
    "Before we begin to implement our MLP NN, let's run a logistic regression again with 8 features, and evaluate the results using the correct time aware cross validation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253561, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "raw_m = np.matrix([short, long, mon, tue, wed, thurs, fri, sat, sun]).T\n",
    "m = m[:idx_split, :]\n",
    "x_data_kbest = SelectKBest(f_classif, k=4).fit_transform(m, y_train)\n",
    "x_data_kbest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_X_train = hstack([full_sites_sparse[:idx_split, :], x_data_kbest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional matrix of the features that we would like to add\n",
    "m1 = np.matrix(time_df[['morning', 'day', 'evening', 'night']])\n",
    "m2 = np.matrix([short, long, mon, wed]).T\n",
    "\n",
    "# Stack the matrices together\n",
    "f_full_sites_sparse = hstack([full_sites_sparse, m1, m2], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate our training and our test test\n",
    "X_train = f_full_sites_sparse[:idx_split, :]\n",
    "X_test = f_full_sites_sparse[idx_split:, :]\n",
    "\n",
    "# Set up our logistic regression model \n",
    "lr = LogisticRegression(C=1.0, solver='lbfgs',\n",
    "                       random_state=17).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.90078713, 0.8250601 , 0.93537853, 0.98023751, 0.91629899,\n",
      "       0.96431189, 0.94736491, 0.9524913 , 0.8834486 , 0.96206353]), 0.9267442484537618) (array([0.8597026 , 0.72166563, 0.88656256, 0.96148235, 0.87898207,\n",
      "       0.93132678, 0.93054671, 0.91691339, 0.81895597, 0.94242813]), 0.884856618974835)\n"
     ]
    }
   ],
   "source": [
    "# Set up the correct time aware cross validation scheme\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "cv_scores_1 = cross_val_score(lr, X_train, y_train, cv=time_split,\n",
    "                           scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "cv_scores_2 = cross_val_score(lr, kbest_X_train, y_train, cv=time_split,\n",
    "                             scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print((cv_scores_1, cv_scores_1.mean()), (cv_scores_2, cv_scores_2.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameters tuning\n",
    "param_grid = {'C': np.logspace(-2, 2, 10)}\n",
    "logit_grid_searcher = GridSearchCV(lr, param_grid=param_grid,\n",
    "                                  scoring='roc_auc', n_jobs=-1,\n",
    "                                  cv=time_split, verbose=1)\n",
    "\n",
    "logit_grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5994842503189409} 0.9260464779265046\n"
     ]
    }
   ],
   "source": [
    "print(logit_grid_searcher.best_params_, logit_grid_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our shallow neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall attempt to train our shallow neural network using Keras+Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load some important packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import metrics\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take every 12th data to reduce the size of training set\n",
    "nn_X_train = X_train[::12, :]\n",
    "nn_y_train = y_train[::12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21131, 48379)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=48379, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                 metrics=[metrics.categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 1380/21131 [>.............................] - ETA: 42s - loss: 0.5157 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-10929288092b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# Fit the model\n",
    "model.fit(nn_X_train, nn_y_train, epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate hold out set\n",
    "nn_X_test = X_train[2::12, :]\n",
    "nn_y_test = y_train[2::12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_val_score(model, X_train, y_train,\n",
    "                           cv=time_split, scoring='roc_auc',\n",
    "                           n_jobs=1)\n",
    "print(cross_val, np.mean(cross_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82797/82797 [==============================] - 24s 285us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "write_to_submission_file(predictions, 'baseline_16.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran the MLP with all our training data, we managed to score 0.92. However, running it with a smaller training set got us a score of 0.58, a vast drop in performance for our neural network.\n",
    "\n",
    "It seems like having more data is indeed more important... However, it took us 3 minutes per epoch when training the MLP using the complete training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our Neural Network to run faster, let's try pruning some of the less important features. In this case, we will be removing the sites that have only been visited once during the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26317"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the number of times that a site appears in the training set \n",
    "site_count = full_sites_sparse[:idx_split, :].sum(axis=0).tolist()[0]\n",
    "keep_index = [ind for ind, x in enumerate(site_count) if x > 1]\n",
    "len(keep_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 26317) (253561, 48379)\n"
     ]
    }
   ],
   "source": [
    "# Remove the sites that have only been visited once in the training set\n",
    "pruned_nn_X_train = X_train[:, keep_index]\n",
    "print(pruned_nn_X_train.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=26317, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                 metrics=[metrics.categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "253561/253561 [==============================] - 150s 593us/step - loss: 0.0444 - categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "253561/253561 [==============================] - 150s 591us/step - loss: 0.0322 - categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "253561/253561 [==============================] - 150s 590us/step - loss: 0.0315 - categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "253561/253561 [==============================] - 148s 584us/step - loss: 0.0310 - categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "253561/253561 [==============================] - 148s 585us/step - loss: 0.0308 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad3b527048>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create the model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# Fit the model\n",
    "model.fit(pruned_nn_X_train, y_train, epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23051/23051 [==============================] - 7s 302us/step - loss: 0.2199 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 180us/step\n",
      "Epoch 1/1\n",
      "46102/46102 [==============================] - 13s 291us/step - loss: 0.1367 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 181us/step\n",
      "Epoch 1/1\n",
      "69153/69153 [==============================] - 20s 289us/step - loss: 0.1474 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 182us/step\n",
      "Epoch 1/1\n",
      "92204/92204 [==============================] - 26s 286us/step - loss: 0.0991 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 187us/step\n",
      "Epoch 1/1\n",
      "115255/115255 [==============================] - 33s 288us/step - loss: 0.0776 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 188us/step\n",
      "Epoch 1/1\n",
      "138306/138306 [==============================] - 40s 291us/step - loss: 0.0827 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 190us/step\n",
      "Epoch 1/1\n",
      "161357/161357 [==============================] - 46s 286us/step - loss: 0.1296 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 190us/step\n",
      "Epoch 1/1\n",
      "184408/184408 [==============================] - 54s 290us/step - loss: 0.0722 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 187us/step\n",
      "Epoch 1/1\n",
      "207459/207459 [==============================] - 60s 289us/step - loss: 0.0515 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 4s 188us/step\n",
      "Epoch 1/1\n",
      "230510/230510 [==============================] - 69s 300us/step - loss: 0.0488 - categorical_accuracy: 1.0000\n",
      "23051/23051 [==============================] - 5s 196us/step\n",
      "[0.58742378 0.54954618 0.76366229 0.93318354 0.79425275 0.85913611\n",
      " 0.89598052 0.83011927 0.89553738 0.89180542] 0.8000647252461242\n"
     ]
    }
   ],
   "source": [
    "cross_val = cross_val_score(model, pruned_nn_X_train, y_train,\n",
    "                           cv=time_split, scoring='roc_auc',\n",
    "                           n_jobs=1)\n",
    "print(cross_val, np.mean(cross_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to optimize the parameters of our Neural Network using the GridSearchCV method. We will first optimize the epoch number and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [10, 20, 30],\n",
    "             'epochs': [5, 10, 20]}\n",
    "\n",
    "nn_model_grid = GridSearchCV(model, param_grid=param_grid, n_jobs=1,\n",
    "                             scoring='roc_auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Epoch 1/5\n",
      "169040/169040 [==============================] - 99s 587us/step - loss: 0.0412 - categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "169040/169040 [==============================] - 98s 577us/step - loss: 0.0213 - categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "169040/169040 [==============================] - 97s 575us/step - loss: 0.0195 - categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "169040/169040 [==============================] - 99s 584us/step - loss: 0.0187 - categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "169040/169040 [==============================] - 97s 574us/step - loss: 0.0184 - categorical_accuracy: 1.0000\n",
      "84521/84521 [==============================] - 19s 230us/step\n",
      "169040/169040 [==============================] - 39s 230us/step\n",
      "Epoch 1/5\n",
      "169041/169041 [==============================] - 98s 581us/step - loss: 0.0472 - categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "169041/169041 [==============================] - 98s 577us/step - loss: 0.0348 - categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "169041/169041 [==============================] - 97s 575us/step - loss: 0.0336 - categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "169041/169041 [==============================] - 97s 576us/step - loss: 0.0329 - categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "169041/169041 [==============================] - 98s 577us/step - loss: 0.0324 - categorical_accuracy: 1.0000\n",
      "84520/84520 [==============================] - 19s 223us/step\n",
      "169041/169041 [==============================] - 38s 223us/step\n",
      "Epoch 1/5\n",
      "169041/169041 [==============================] - 98s 582us/step - loss: 0.0447 - categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "169041/169041 [==============================] - 98s 581us/step - loss: 0.0326 - categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "169041/169041 [==============================] - 98s 580us/step - loss: 0.0316 - categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "169041/169041 [==============================] - 98s 579us/step - loss: 0.0310 - categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "169041/169041 [==============================] - 98s 579us/step - loss: 0.0306 - categorical_accuracy: 1.0000\n",
      "84520/84520 [==============================] - 19s 226us/step\n",
      "169041/169041 [==============================] - 38s 225us/step\n",
      "Epoch 1/10\n",
      "169040/169040 [==============================] - 98s 581us/step - loss: 0.0379 - categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "169040/169040 [==============================] - 98s 579us/step - loss: 0.0214 - categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "169040/169040 [==============================] - 98s 579us/step - loss: 0.0201 - categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "169040/169040 [==============================] - 98s 578us/step - loss: 0.0192 - categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "169040/169040 [==============================] - 97s 575us/step - loss: 0.0185 - categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "169040/169040 [==============================] - 98s 578us/step - loss: 0.0181 - categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "169040/169040 [==============================] - 97s 576us/step - loss: 0.0177 - categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "169040/169040 [==============================] - 98s 579us/step - loss: 0.0176 - categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "169040/169040 [==============================] - 98s 578us/step - loss: 0.0173 - categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "169040/169040 [==============================] - 97s 575us/step - loss: 0.0172 - categorical_accuracy: 1.0000\n",
      "84521/84521 [==============================] - 20s 239us/step\n",
      "169040/169040 [==============================] - 40s 238us/step\n",
      "Epoch 1/10\n",
      "169041/169041 [==============================] - 98s 580us/step - loss: 0.0487 - categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "169041/169041 [==============================] - 98s 578us/step - loss: 0.0349 - categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "169041/169041 [==============================] - 101s 597us/step - loss: 0.0336 - categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "169041/169041 [==============================] - 105s 621us/step - loss: 0.0329 - categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "169041/169041 [==============================] - 106s 627us/step - loss: 0.0324 - categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "169041/169041 [==============================] - 105s 623us/step - loss: 0.0320 - categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "169041/169041 [==============================] - 105s 622us/step - loss: 0.0318 - categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "169041/169041 [==============================] - 104s 617us/step - loss: 0.0316 - categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "169041/169041 [==============================] - 104s 616us/step - loss: 0.0314 - categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "169041/169041 [==============================] - 104s 613us/step - loss: 0.0313 - categorical_accuracy: 1.0000\n",
      "84520/84520 [==============================] - 20s 242us/step\n",
      "169041/169041 [==============================] - 40s 237us/step\n",
      "Epoch 1/10\n",
      "169041/169041 [==============================] - 103s 610us/step - loss: 0.0538 - categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "169041/169041 [==============================] - 103s 611us/step - loss: 0.0336 - categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "169041/169041 [==============================] - 103s 608us/step - loss: 0.0324 - categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "169041/169041 [==============================] - 103s 609us/step - loss: 0.0317 - categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "169041/169041 [==============================] - 103s 610us/step - loss: 0.0314 - categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "169041/169041 [==============================] - 103s 611us/step - loss: 0.0312 - categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "169041/169041 [==============================] - 103s 610us/step - loss: 0.0310 - categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "169041/169041 [==============================] - 103s 609us/step - loss: 0.0308 - categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "169041/169041 [==============================] - 103s 611us/step - loss: 0.0308 - categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "169041/169041 [==============================] - 103s 609us/step - loss: 0.0307 - categorical_accuracy: 1.0000\n",
      "84520/84520 [==============================] - 20s 239us/step\n",
      "169041/169041 [==============================] - 39s 232us/step\n",
      "Epoch 1/20\n",
      "169040/169040 [==============================] - 100s 593us/step - loss: 0.0958 - categorical_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "169040/169040 [==============================] - 103s 607us/step - loss: 0.0317 - categorical_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "169040/169040 [==============================] - 100s 590us/step - loss: 0.0262 - categorical_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "169040/169040 [==============================] - 100s 593us/step - loss: 0.0247 - categorical_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "169040/169040 [==============================] - 103s 610us/step - loss: 0.0238 - categorical_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "169040/169040 [==============================] - 101s 596us/step - loss: 0.0233 - categorical_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "169040/169040 [==============================] - 100s 594us/step - loss: 0.0228 - categorical_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "169040/169040 [==============================] - 98s 579us/step - loss: 0.0224 - categorical_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "104980/169040 [=================>............] - ETA: 36s - loss: 0.0215 - categorical_accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "nn_model_grid.fit(pruned_nn_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
